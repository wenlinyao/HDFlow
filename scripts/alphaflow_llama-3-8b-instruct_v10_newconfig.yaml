
# The path to the base model's checkpoint directory to load for finetuning. (type: <class 'Path'>, default: checkpoints/stabilityai/stablelm-base-alpha-3b)
checkpoint_dir: checkpoints/meta-llama/Meta-Llama-3-8B-Instruct

# Directory in which to save checkpoints and logs. (type: <class 'Path'>, default: out/finetune/full)
out_dir: /apdcephfs/share_300000800/user/wenlinyao/research2/AlphaCode/litgpt_trained_model/llama-3-8b-instruct_4k_alphaflow_v10


# The precision to use for finetuning. Possible choices: "bf16-true", "bf16-mixed", "32-true". (type: Optional[str], default: null)
precision: bf16-true

# How many devices/GPUs to use (type: Union[int, str], default: 1)
devices: 8

# Path to a checkpoint directory to resume from in case training was interrupted, or ``True`` to resume
# from the latest checkpoint in ``out_dir``. (type: Union[bool, Path], default: False)
resume: false

# Data-related arguments. If not provided, the default is ``litgpt.data.Alpaca``.
data:
  class_path: litgpt.data.JSON
  init_args:
    json_path: /apdcephfs/share_300000800/user/wenlinyao/research2/AlphaCode/litgpt_data/alphaflow_training_data_v10.json
    val_split_fraction: 0.005
    prompt_style: llama3
    seed: 42
    num_workers: 4

# Training-related arguments. See ``litgpt.args.TrainArgs`` for details
train:

  # Number of optimizer steps between saving checkpoints (type: Optional[int], default: 1000)
  save_interval: 600

  # Number of iterations between logging calls (type: int, default: 1)
  log_interval: 1
  
  # TP_size * PP_size * DP_size == num_GPUs
  # num_of_micro_batch = global_batch_size / micro_batch_size
  # Number of samples between optimizer steps across data-parallel ranks (type: int, default: 64)
  global_batch_size: 128

  # Number of samples per data-parallel rank (type: int, default: 1)
  micro_batch_size: 1

  # Number of iterations with learning rate warmup active (type: int, default: 100)
  lr_warmup_steps: 50

  # Number of epochs to train on (type: Optional[int], default: 5)
  epochs: 4

  # Total number of tokens to train on (type: Optional[int], default: null)
  max_tokens:

  # Limits the number of optimizer steps to run. (type: Optional[int], default: null)
  max_steps:

  # Limits the length of samples. Off by default (type: Optional[int], default: null)
  max_seq_length: 4096

  # Whether to tie the embedding weights with the language modeling head weights. (type: Optional[bool], default: null)
  tie_embeddings:

  #   (type: Optional[float], default: null)
  max_norm:

  #   (type: float, default: 6e-05)
  min_lr: 2.0e-06

# Evaluation-related arguments. See ``litgpt.args.EvalArgs`` for details
eval:

  # Number of optimizer steps between evaluation calls (type: int, default: 600)
  interval: 200

  # Number of tokens to generate (type: Optional[int], default: 100)
  max_new_tokens: 2048

  # Number of iterations (type: int, default: 100)
  max_iters: 100

# The name of the logger to send metrics to. (type: Literal['wandb', 'tensorboard', 'csv'], default: csv)
logger_name: csv

# The random seed to use for reproducibility. (type: int, default: 1337)
seed: 11


# Optimizer-related arguments
optimizer:

  class_path: torch.optim.AdamW
  
  init_args:
    
    #   (type: float, default: 0.001)
    lr: 2.0e-5
    
    #   (type: float, default: 0.01)
    weight_decay: 0.02
    
    #   (type: tuple, default: (0.9,0.999))
    betas:
      - 0.9
      - 0.95